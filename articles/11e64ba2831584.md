---
title: "OllamaとStrands Agentsで無料AIエージェントを動かしてみた！"
emoji: "🕌"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["ai","ollama","strandsagents","mcp"]
published: true
---


## はじめに

AIエージェントについて調査する機会がありStrands Agentsを試してみました！    
Strands AgentsはAWSが提供していることもあり、Bedrockを使用するのが一般的ですが他のプロバイダーでも利用できます。  
今回は手軽にローカル環境で使用できるOllamaで試してみました。  

https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/ollama/

## Ollamaセットアップ

### Ollamaインストール

筆者の環境はWindowsでWSL環境になります。  
Dockerで利用する方法もありますが今回はWSLに直接インストールで利用します。  
※WSLはセットアップ済み想定

下記コマンドでインストールができます。
```
curl -fsSL https://ollama.com/install.sh | sh
```
https://ollama.com/download/linux

### モデルの追加

モデル一覧から希望のモデルをプルします。  
https://ollama.com/library/llama3.2  
例ではllama3.2:3bをインストールしています。

```
ollama pull llama3.2:3b
```

プルしたあとにlistコマンドで現在インストール済みのモデル一覧が確認できます。  
インストールしたモデルが表示されていれば問題ありません。

```
ollama list
```

### Ollamaの起動

Ollamaを起動させます。
起動が確認できたらOllamaのセットアップは完了です。

```
ollama serve
```


## Strands Agentsのセットアップ

### パッケージのインストール

uvを使って必要なパッケージをインストールします。  
pipでも問題ありません。
```
uv add strands-agents[ollama] strands-agents-tools
```

### コード作成

簡単にやり取りするためのコード例です。
```python
from strands import Agent
from strands.models.ollama import OllamaModel

ollama_model = OllamaModel(
    host="http://localhost:11434",
    model_id="llama3.2:3b"
)

agent = Agent(model=ollama_model)

agent("こんにちは、日本語で返答して")
```

これだけ記載してpython ファイル名で動作します。  
返答結果
```
こんにちは！どういたしまして。日本語で回答することになりますので、ぜひ質問をください。
```

### オプション設定

temperatureやtop_p,top_kの指定も可能です。  
デフォルトではNoneとなっているため要件に合わせて調整が可能です。
- temperature
	- LLMが次の単語を予測する際の「ランダム性」を制御するパラメータのこと
	- 値が低いほど確率の高い単語が選ばれ、値が高いとランダム性が上がる
	- 確率分布そのものの形を変える操作
- Top-p
	- モデルが応答を生成する際の決定性をコントロールすることができる。
	- 確率の高い順に並べて、設定値（確率値）に合わせて候補を絞る 
	- どの候補まで見るかパーセントで決める
- Top-k
	- 決めた値に候補を絞る 
	- どの候補まで見るか数で決める

temperatureを上げると返答に英語が含まれたりするようになります。
```
こんにちは！どうぞ。何をASKしますか？
```

## MCPの使用

MCPとの連携もできます。
今回はAWSのknowledgeを使用して検証します。
https://github.com/awslabs/mcp/tree/main/src/aws-knowledge-mcp-server

下記がコード例です。
```python
from strands import Agent
from strands.models.ollama import OllamaModel
from strands.tools.mcp import MCPClient
from mcp.client.streamable_http import streamablehttp_client

ollama_model = OllamaModel(
    host="http://localhost:11434",
    model_id="llama3.2:3b",
    # model_id="gemma3",
    temperature=0.7
)

mcp = MCPClient(
    lambda: streamablehttp_client(
        "https://knowledge-mcp.global.api.aws"
    )
)

with mcp:
    agent = Agent(model=ollama_model, tools=mcp.list_tools_sync())
    response = agent("EC2の最新情報を教えて、日本語で返答して")
    print(response) 
```

注意点としてOllamaで使用するモデルはツールに対応している必要があります。  
コメントアウトしているgemma3はツールに対応していないため、実行すると下記のようなエラーが表示されます。  
ツールに対応したモデルを選択するように注意してください。  
```
ollama._types.ResponseError: registry.ollama.ai/library/gemma3:latest does not support tools (status code: 400)
```

## さいごに

Strands AgentsでOllamaを使ってさくっと動かしてみました。  
記載している例はかなり簡単なもので意味のないAIエージェントですが、MCP連携など簡単にでき、なんとなくの概要をつかめることができました。  
Ollamaを使うことでモデル使用料が掛からないため手軽に色々試せるところがメリットだと思います！  
参考にしていただければ幸いです！
